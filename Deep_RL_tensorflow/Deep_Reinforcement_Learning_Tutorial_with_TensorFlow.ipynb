{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deep Reinforcement Learning Tutorial with TensorFlow\n",
    "\n",
    "본 튜토리얼은 Tensorflow 환경에서 DeeP Reinforcement Learning에 대한 기초적인 실습을 하기 위한 자료이다. 첫 번째 파트에서는 tensorflow에 대한 기초적인 실습과 MLP 예제를 다루어보고, 두 번째 파트에서는 Karpathy가 오픈소스로 공개한 Deep Reinforcement Learning 예제를 분석하고 직접 수정해보는 시간을 갖는다.\n",
    "\n",
    "\n",
    "The code and comments are written by Dong-Hyun Kwak (imcomking@gmail.com)\n",
    "\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. TensorFlow\n",
    "\n",
    "TensorFlow 이하, tf는 구글 주도하에 초기 개발되고, 현재 오픈소스로 공개되어 널리 쓰이고 있는 분산 기계학습(딥러닝)을 위한 라이브러리이다. Computational Graph를 사용한 Theano의 장점을 그대로 살려 automatic derivation(자동 미분)이 가능하고, Spark처럼 분산 클라우드 컴퓨팅 환경에서 동작하기 위한 아키텍처로 설계되었다.\n",
    "\n",
    "우선 MLP 예제 코드를 통해 tf의 동작을 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "Multi-Layer Perceptron, 이하 MLP는 다음과 같은 구조를 가진 모델이다. Convolutional Neural Networks와 달리 굉장히 layer간의 연결이 빽빽하게 가득 차 있어, dense layer 혹은 fully connected layer라는 이름으로도 불리고 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlp 그림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf를 이용해서 MNIST 데이터를 MLP로 분류하는 코드를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy: 0.133\n",
      "step 500, training accuracy: 0.267\n",
      "step 1000, training accuracy: 0.887\n",
      "step 1500, training accuracy: 0.880\n",
      "step 2000, training accuracy: 0.967\n",
      "step 2500, training accuracy: 0.960\n",
      "step 3000, training accuracy: 0.953\n",
      "step 3500, training accuracy: 0.967\n",
      "step 4000, training accuracy: 0.960\n",
      "step 4500, training accuracy: 0.953\n",
      "step 5000, training accuracy: 0.973\n",
      "test accuracy: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP instance at 0x7faedd629098>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self):\n",
    "        # download mnist data from internet\n",
    "        mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "        # open a session which is a set of computation graph operations\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        # placeholder is used fof transferring the data\n",
    "        x = tf.placeholder(\"float\", shape=[None, 784]) # none represents variable length of dimension\n",
    "        y_target = tf.placeholder(\"float\", shape=[None, 10]) # shape argument is optional, but usefule to debug\n",
    "\n",
    "        # Variable is allocated in GPU memory\n",
    "        W1 = tf.Variable(tf.zeros([784, 256]))\n",
    "        b1 = tf.Variable(tf.zeros([256]))\n",
    "        h1 = tf.sigmoid(tf.matmul(x, W1) + b1)\n",
    "        \n",
    "        W2 = tf.Variable(tf.zeros([256, 10]))\n",
    "        b2 = tf.Variable(tf.zeros([10]))\n",
    "        y = tf.nn.softmax(tf.matmul(h1, W2) + b2)\n",
    "        # softmax classification\n",
    "        \n",
    "        # initialize the variables by sess.run. maybe really allocating step?\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # define the Loss function\n",
    "        cross_entropy = -tf.reduce_sum(y_target*tf.log(y))\n",
    "\n",
    "        # define optimization algorithm\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        \n",
    "        # list of boolean which is result of comparing the training prediction & real data\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "        \n",
    "        # change true -> 1 false -> 0 and calc mean.\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        # training step\n",
    "        for i in range(5001): # 1000 step of minibatches\n",
    "            batch = mnist.train.next_batch(150) # 50 is minibatch size\n",
    "            train_step.run(feed_dict={x: batch[0], y_target: batch[1]}) # placeholder's none length is replaced by i:i+100 indexes\n",
    "            \n",
    "            if i%500 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_target: batch[1]})\n",
    "                print \"step %d, training accuracy: %.3f\"%(i, train_accuracy)\n",
    "        \n",
    "        # for given x, y_target data set\n",
    "        print  \"test accuracy: %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_target: mnist.test.labels})\n",
    "\n",
    "        \n",
    "MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard 설정하기\n",
    "TensorFlow는 TensorBoard라는 매우 강력한 visualization tool을 제공한다. 이를 사용하면 웹브라우저 형태로 사용자가 모델의 구조를 눈으로 확인하고, 파라미터 값의 변화를 살펴보는 등의 직관적인 분석이 가능하다.\n",
    "\n",
    "이는 다른 라이브러리에서는 제공하지 않는 tf만의 차별화된 강점이다.\n",
    "\n",
    "이를 활용해 방금 만들었던 MLP에 대해서 분석해보자. 그러려면 다음의 사항을 반영해 코드를 수정하여야 한다.\n",
    "\n",
    "* **변수들의 이름 지어주기**\n",
    "\n",
    "* **변수들의 Summary 생성**\n",
    "\n",
    "* **변수들의 Summary 기록**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy: 0.133\n",
      "step 500, training accuracy: 0.420\n",
      "step 1000, training accuracy: 0.947\n",
      "step 1500, training accuracy: 0.967\n",
      "step 2000, training accuracy: 0.960\n",
      "step 2500, training accuracy: 0.953\n",
      "step 3000, training accuracy: 0.960\n",
      "step 3500, training accuracy: 0.987\n",
      "step 4000, training accuracy: 0.953\n",
      "step 4500, training accuracy: 0.987\n",
      "step 5000, training accuracy: 0.973\n",
      "test accuracy: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP_tf_board instance at 0x7fc1244fdea8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class MLP_tf_board():\n",
    "    def __init__(self):\n",
    "        # download mnist data from internet\n",
    "        mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "        # open a session which is a set of computation graph operations\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        # placeholder is used fof transferring the data\n",
    "        x = tf.placeholder(\"float\", shape=[None, 784], name = 'x') # none represents variable length of dimension\n",
    "        y_target = tf.placeholder(\"float\", shape=[None, 10], name = 'y_target') # shape argument is optional, but usefule to debug\n",
    "\n",
    "        # Variable is allocated in GPU memory\n",
    "        W1 = tf.Variable(tf.zeros([784, 256]), name = 'W1')\n",
    "        b1 = tf.Variable(tf.zeros([256]), name = 'b1')\n",
    "        h1 = tf.sigmoid(tf.matmul(x, W1) + b1, name = 'h1')\n",
    "        \n",
    "        W2 = tf.Variable(tf.zeros([256, 10]), name = 'W2')\n",
    "        b2 = tf.Variable(tf.zeros([10]), name = 'b2')\n",
    "        y = tf.nn.softmax(tf.matmul(h1, W2) + b2, name = 'y')\n",
    "        # softmax classification\n",
    "        \n",
    "        # initialize the variables by sess.run. maybe really allocating step?\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "        # define the Loss function\n",
    "        cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "\n",
    "        # define optimization algorithm\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        \n",
    "        # list of boolean which is result of comparing the training prediction & real data\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "        \n",
    "        # change true -> 1 false -> 0 and calc mean.\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # create summary of parameters\n",
    "        tf.histogram_summary('weights_1', W1)\n",
    "        tf.histogram_summary('weights_2', W2)\n",
    "        tf.histogram_summary('y', y)\n",
    "        tf.scalar_summary('cross_entropy', cross_entropy)\n",
    "        merged = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter(\"/tmp/mlp\" , sess.graph_def)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # training step\n",
    "        for i in range(5001): # 1000 step of minibatches\n",
    "            batch = mnist.train.next_batch(150) # 50 is minibatch size\n",
    "            train_step.run(feed_dict={x: batch[0], y_target: batch[1]}) # placeholder's none length is replaced by i:i+100 indexes\n",
    "            \n",
    "            if i%500 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_target: batch[1]})\n",
    "                print \"step %d, training accuracy: %.3f\"%(i, train_accuracy)\n",
    "                \n",
    "                # calculate the summary and write.\n",
    "                summary = sess.run(merged, feed_dict={x:batch[0], y_target: batch[1]})\n",
    "                summary_writer.add_summary(summary , i)\n",
    "        \n",
    "        # for given x, y_target data set\n",
    "        print  \"test accuracy: %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_target: mnist.test.labels})\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "MLP_tf_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard 실행하기\n",
    "위와 같이 코드를 수정했다면, 이제 리눅스 shell에서 tensorboard.py가 있는 곳으로 이동한 다음을 실행한다.\n",
    "\n",
    "\n",
    "cd tensorflow/tensorflow/tensorboard\n",
    "\n",
    "python tensorboard.py --logdir=/tmp/mlp\n",
    "\n",
    "\n",
    "그다음 0.0.0.0:6006/#graphs 에 접속하면 아래와 같은 그림을 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"tensorboard_mlp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Deep Reinforcement Learing\n",
    "\n",
    "Deep Reinforcement Learning이란, 기존의 강화학습에서 사용하는 Q function을 딥러닝으로 근사하여 사용하는 모델을 의미한다. 대표적으로 구글 Deep Mind의 Atari와 최근에 화제가 된 AlphaGo 역시 이 Deep Reinforcement Learning을 이용한 응용의 한가지이다.\n",
    "\n",
    "이번 파트에서는 Deep Reinforcement Learning을 이용해서 간단한 2차원 게임을 플레이하고, reward로 부터 스스로 학습하는 Kaparthy의 오픈소스 예제를 실습해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Deep RL\" style=\"border-width:0\" width=\"600\" src=\"example.gif?raw=true\" />\n",
    "\n",
    "\n",
    "(출처: https://github.com/nivwusquorum/tensorflow-deepq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning, 이하 RL은 supervised learning과 달리 주어진 데이터에 대한 정확한 정답을 제공받지 않고, 내가 한 행동에 대한 reward feedback 만으로 학습을 수행하는 알고리즘이다. 이를 강화학습이라 부르며, 이것을 수행하는 가장 대표적인 알고리즘으로 Q-Learning 이 있다.\n",
    "\n",
    "우선 RL에서 사용하는 용어와 개념을 이해해보자.\n",
    "\n",
    "* **numpy**: scientific computing (matrix, algebra, random, etc.)\n",
    "* **matplotlib**: plotting\n",
    "* **h5py, cPickle**: efficient data saving and loading\n",
    "* **tensorflow**: GPU and symbolic computing\n",
    "* **IPython interact**: scrolled view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Import Libraries\n",
    "We are going to need the following libraries:\n",
    "* **numpy**: scientific computing (matrix, algebra, random, etc.)\n",
    "* **matplotlib**: plotting\n",
    "* **h5py, cPickle**: efficient data saving and loading\n",
    "* **tensorflow**: GPU and symbolic computing\n",
    "* **IPython interact**: scrolled view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "\n",
    "#from tf_rl.models import MLP\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Settings\n",
    "\n",
    "이제 우리가 원하는 게임 환경을 설정하고, 적절한 reward와 object의 개수 및 observation 을 조절한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/nacsi\n"
     ]
    }
   ],
   "source": [
    "current_settings = {\n",
    "    'objects': [\n",
    "        'friend',\n",
    "        'enemy',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'hero':   'yellow',\n",
    "        'friend': 'green',\n",
    "        'enemy':  'red',\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'friend': 0.1,\n",
    "        'enemy': -0.1,\n",
    "    },\n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (700,500),\n",
    "    'hero_initial_position': [400, 300],\n",
    "    'hero_initial_speed':    [0,   0],\n",
    "    \"maximum_speed\":         [50, 50],\n",
    "    \"object_radius\": 10.0,\n",
    "    \"num_objects\": {\n",
    "        \"friend\" : 25,\n",
    "        \"enemy\" :  25,\n",
    "    },\n",
    "    \"num_observation_lines\" : 32, # the number of antennas\n",
    "    \"observation_line_length\": 240., # the length of antennas\n",
    "    \"tolerable_distance_to_wall\": 50, \n",
    "    \"wall_distance_penalty\":  -0.0, # if the hero is close to wall, that receives penalty\n",
    "    \"delta_v\": 50 # speed value\n",
    "}\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n",
    "\n",
    "# this LOG_DIR is used for tensorboard\n",
    "LOG_DIR = \"/tmp/nacsi\"\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Architecture\n",
    "\n",
    "이제 Q function을 근사하기 위한 딥러닝 모델을 만들어보자. 이번 예제에서는 위에서 보았던 4층짜리 MLP를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tf_rl.utils import base_name\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, input_sizes, output_size, scope):\n",
    "        \"\"\"Cretes a neural network layer.\"\"\"\n",
    "        if type(input_sizes) != list:\n",
    "            input_sizes = [input_sizes]\n",
    "\n",
    "        self.input_sizes = input_sizes\n",
    "        self.output_size = output_size\n",
    "        self.scope       = scope or \"Layer\"\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            self.Ws = []\n",
    "            for input_idx, input_size in enumerate(input_sizes):\n",
    "                W_name = \"W_%d\" % (input_idx,)\n",
    "                W_initializer =  tf.random_uniform_initializer(\n",
    "                        -1.0 / math.sqrt(input_size), 1.0 / math.sqrt(input_size))\n",
    "                W_var = tf.get_variable(W_name, (input_size, output_size), initializer=W_initializer)\n",
    "                self.Ws.append(W_var)\n",
    "            self.b = tf.get_variable(\"b\", (output_size,), initializer=tf.constant_initializer(0))\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        if type(xs) != list:\n",
    "            xs = [xs]\n",
    "        assert len(xs) == len(self.Ws), \\\n",
    "                \"Expected %d input vectors, got %d\" % (len(self.Ws), len(xs))\n",
    "        with tf.variable_scope(self.scope):\n",
    "            return sum([tf.matmul(x, W) for x, W in zip(xs, self.Ws)]) + self.b\n",
    "\n",
    "    def variables(self):\n",
    "        return [self.b] + self.Ws\n",
    "\n",
    "    def copy(self, scope=None):\n",
    "        scope = scope or self.scope + \"_copy\"\n",
    "\n",
    "        with tf.variable_scope(scope) as sc:\n",
    "            for v in self.variables():\n",
    "                tf.get_variable(base_name(v), v.get_shape(),\n",
    "                        initializer=lambda x,dtype=tf.float32: v.initialized_value())\n",
    "            sc.reuse_variables()\n",
    "            return Layer(self.input_sizes, self.output_size, scope=sc)\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, input_sizes, hiddens, nonlinearities, scope=None, given_layers=None):\n",
    "        self.input_sizes = input_sizes\n",
    "        # observation is 5 features(distance of each object and X,Y speed) of closest 32 object with hero(friend, enemy, wall) + 2 hero's own speed X,Y\n",
    "        # ==> 5*32 + 2 = 162 features about the game\n",
    "        self.hiddens = hiddens\n",
    "        self.input_nonlinearity, self.layer_nonlinearities = nonlinearities[0], nonlinearities[1:]\n",
    "        self.scope = scope or \"MLP\"\n",
    "\n",
    "        assert len(hiddens) == len(nonlinearities), \\\n",
    "                \"Number of hiddens must be equal to number of nonlinearities\"\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            if given_layers is not None:\n",
    "                self.input_layer = given_layers[0]\n",
    "                self.layers      = given_layers[1:]\n",
    "            else:\n",
    "                self.input_layer = Layer(input_sizes, hiddens[0], scope=\"input_layer\") # 135 -> 200\n",
    "                self.layers = []\n",
    "\n",
    "                for l_idx, (h_from, h_to) in enumerate(zip(hiddens[:-1], hiddens[1:])): # hiddens == [200, 200, 4], so this mean, swifting the index by 1\n",
    "                    # (200, 200) , (200,4)\n",
    "                    self.layers.append(Layer(h_from, h_to, scope=\"hidden_layer_%d\" % (l_idx,)))\n",
    "                    # this has 4 layers\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        if type(xs) != list:\n",
    "            xs = [xs]\n",
    "        with tf.variable_scope(self.scope):\n",
    "            hidden = self.input_nonlinearity(self.input_layer(xs))\n",
    "            for layer, nonlinearity in zip(self.layers, self.layer_nonlinearities):\n",
    "                hidden = nonlinearity(layer(hidden))\n",
    "            return hidden\n",
    "\n",
    "    def variables(self):\n",
    "        res = self.input_layer.variables()\n",
    "        for layer in self.layers:\n",
    "            res.extend(layer.variables())\n",
    "        return res\n",
    "\n",
    "    def copy(self, scope=None):\n",
    "        scope = scope or self.scope + \"_copy\"\n",
    "        nonlinearities = [self.input_nonlinearity] + self.layer_nonlinearities\n",
    "        given_layers = [self.input_layer.copy()] + [layer.copy() for layer in self.layers]\n",
    "        return MLP(self.input_sizes, self.hiddens, nonlinearities, scope=scope,\n",
    "                given_layers=given_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make an Agent\n",
    "\n",
    "이제 Discrete Deep Q learning 알고리즘이 이 게임을 플레이하면서 학습을 하도록 agent로 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow business - it is always good to reset a graph before creating a new controller.\n",
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This little guy will let us run tensorboard\n",
    "#      tensorboard --logdir [LOG_DIR]\n",
    "journalist = tf.train.SummaryWriter(LOG_DIR)\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden\n",
    "# layers\n",
    "brain = MLP([g.observation_size,], [200, 200, g.num_actions], \n",
    "            [tf.tanh, tf.tanh, tf.identity])\n",
    "\n",
    "# The optimizer to use. Here we use RMSProp as recommended\n",
    "# by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(g.observation_size, g.num_actions, brain, optimizer, session,\n",
    "                                   discount_rate=0.99, exploration_period=5000, max_experience=10000, \n",
    "                                   store_every_nth=4, train_every_nth=4,\n",
    "                                   summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "# graph was not available when journalist was created  \n",
    "journalist.add_graph(session.graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play the Game\n",
    "\n",
    "실제로 게임을 플레이하면서 강화학습을 하는 과정을 지켜본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<?xml version=\"1.0\"?>\n",
       "\n",
       "<svg height=\"600\" width=\"720\" >\n",
       "\n",
       " <g style=\"fill-opacity:1.0; stroke:black;\n",
       "\n",
       "  stroke-width:1;\">\n",
       "\n",
       "  <rect x=\"10\" y=\"10\" height=\"500\"\n",
       "\n",
       "        width=\"700\" style=\"fill:none;\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"860\" y2=\"488\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"856\" y2=\"535\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"842\" y2=\"580\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"820\" y2=\"621\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"790\" y2=\"658\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"754\" y2=\"687\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"712\" y2=\"710\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"667\" y2=\"723\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"620\" y2=\"728\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"573\" y2=\"723\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"528\" y2=\"710\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"487\" y2=\"687\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"451\" y2=\"658\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"421\" y2=\"621\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"399\" y2=\"580\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"385\" y2=\"535\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"380\" y2=\"488\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"385\" y2=\"441\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"399\" y2=\"396\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"421\" y2=\"355\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"451\" y2=\"318\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"487\" y2=\"288\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"528\" y2=\"266\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"573\" y2=\"252\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"620\" y2=\"248\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"667\" y2=\"252\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"712\" y2=\"266\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"754\" y2=\"288\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"790\" y2=\"318\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"820\" y2=\"355\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"842\" y2=\"396\" />\n",
       "\n",
       "  <line x1=\"620\" y1=\"488\" x2=\"856\" y2=\"441\" />\n",
       "\n",
       "  <circle cx=\"20\" cy=\"283\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"632\" cy=\"323\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"381\" cy=\"314\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"609\" cy=\"327\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"21\" cy=\"235\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"141\" cy=\"458\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"132\" cy=\"179\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"346\" cy=\"480\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"299\" cy=\"76\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"163\" cy=\"137\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"491\" cy=\"143\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"80\" cy=\"421\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"633\" cy=\"207\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"100\" cy=\"238\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"466\" cy=\"264\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"333\" cy=\"342\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"288\" cy=\"287\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"625\" cy=\"164\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"424\" cy=\"285\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"584\" cy=\"137\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"341\" cy=\"269\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"318\" cy=\"168\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"201\" cy=\"367\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"459\" cy=\"375\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"88\" cy=\"299\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"652\" cy=\"398\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"562\" cy=\"282\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"489\" cy=\"386\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"409\" cy=\"343\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"485\" cy=\"33\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"575\" cy=\"207\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"28\" cy=\"70\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"395\" cy=\"357\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"476\" cy=\"128\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"190\" cy=\"326\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"638\" cy=\"243\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"171\" cy=\"186\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"593\" cy=\"363\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"660\" cy=\"341\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"104\" cy=\"275\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"369\" cy=\"183\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"354\" cy=\"270\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"358\" cy=\"337\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"44\" cy=\"488\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"401\" cy=\"238\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"620\" cy=\"121\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"249\" cy=\"227\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"92\" cy=\"475\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"306\" cy=\"39\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"76\" cy=\"328\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"620\" cy=\"488\" r=\"10\"\n",
       "\n",
       "          style=\"fill:yellow;\" />\n",
       "\n",
       "  <text x=\"10\" y=\"535\" font-size=\"15\">\n",
       "\n",
       "   fps = 120.5\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"555\" font-size=\"15\">\n",
       "\n",
       "   nearest wall = 11.7\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"575\" font-size=\"15\">\n",
       "\n",
       "   reward       = 0.0\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"595\" font-size=\"15\">\n",
       "\n",
       "   objects eaten => enemy: 3, friend: 1\n",
       "\n",
       "  </text>\n",
       "\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<tf_rl.utils.svg.Scene instance at 0x7fc103c74908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = True\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 20\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "    \n",
    "try:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        simulate(simulation=g,\n",
    "                 controller=current_controller,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=0.001,\n",
    "                 save_path=None)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contest!\n",
    "자 이제 위의 여러가지 환경 설정과 모델의 구조를 바꾸어 enemy:friend 의 차이가 최대한 많이 나도록 나의 agent를 학습시켜본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
