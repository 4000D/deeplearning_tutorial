{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(name, shape, init='he', range=None):\n",
    "    \"\"\" Initializes weight.\n",
    "    :param name: Variable name\n",
    "    :param shape: Tensor shape\n",
    "    :param init: Init mode. xavier / normal / uniform / he (default is 'he')\n",
    "    :param range:\n",
    "    :return: Variable\n",
    "    \"\"\"\n",
    "    initializer = tf.constant_initializer()\n",
    "    if init == 'xavier':\n",
    "        fan_in, fan_out = _get_dims(shape)\n",
    "        range = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "        initializer = tf.random_uniform_initializer(-range, range)\n",
    "\n",
    "    elif init == 'he':\n",
    "        fan_in, _ = shape\n",
    "        std = math.sqrt(2.0 / fan_in)\n",
    "        initializer = tf.random_normal_initializer(stddev=std)\n",
    "\n",
    "    elif init == 'normal':\n",
    "        initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "    elif init == 'uniform':\n",
    "        if range is None:\n",
    "            raise ValueError(\"range must not be None if uniform init is used.\")\n",
    "        initializer = tf.random_uniform_initializer(-range, range)\n",
    "\n",
    "    var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    tf.add_to_collection('l2', tf.nn.l2_loss(var))  # Add L2 Loss\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_marks(strIn):\n",
    "    marks = ['.', ',', '-', '!', '?', '\\\"']\n",
    "    \n",
    "    result = strIn\n",
    "    for mark in marks:\n",
    "        result = result.replace(mark, '')\n",
    "    \n",
    "    result = result.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def read_story():\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open('stories.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            chunks = line.split('\\t')\n",
    "            sentences.append( remove_marks(chunks[0]) )\n",
    "            labels.append(int(chunks[1]))\n",
    "            \n",
    "    return sentences, labels\n",
    "\n",
    "def load_glove(dim):\n",
    "    word2vec = {}\n",
    "    with open( \"glove.6B.\" + str(dim) + \"d.txt\") as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            word2vec [l[0]] = map( float, l[1:])\n",
    "    return word2vec\n",
    "\n",
    "def create_vector(word, word2vec, word_vector_size):\n",
    "    vector = np.random.uniform( 0.0, 1.0, (word_vector_size,) )\n",
    "    word2vec[word] = vector\n",
    "    return vector\n",
    "\n",
    "def process_word(word, word2vec, vocab, ivocab, word_vector_size):\n",
    "    if not word in word2vec:\n",
    "        create_vector(word, word2vec, word_vector_size)\n",
    "    if not word in vocab:\n",
    "        next_index = len(vocab)\n",
    "        vocab[word] = next_index\n",
    "        ivocab[next_index] = word\n",
    "        \n",
    "    return word2vec[word]\n",
    "    \n",
    "def make_input_vector(sentences, word2vec, vocab, ivocab, word_vector_size):\n",
    "    inputs = []\n",
    "    num_words = []\n",
    "    for line in sentences:\n",
    "        inp = line.lower().split(' ')\n",
    "        inp = [ w for w in inp if len(w) > 0]\n",
    "        \n",
    "        inp_vector = [process_word( word = w,\n",
    "                                  word2vec = word2vec,\n",
    "                                  vocab = vocab,\n",
    "                                  ivocab = ivocab,\n",
    "                                  word_vector_size = word_vector_size) for w in inp]\n",
    "        inp_vector = np.vstack((inp_vector))\n",
    "        #print inp_vector.shape\n",
    "        inputs.append( inp_vector )\n",
    "        num_words.append(len(inp))\n",
    "        #print len(vocab), len(ivocab)\n",
    "        \n",
    "    return inputs, num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_glove\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_vector_size = 100\n",
    "assert word_vector_size in [50, 100, 200, 300]\n",
    "\n",
    "print 'load_glove'\n",
    "word2vec = load_glove(word_vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch():\n",
    "        \"\"\" Vectorizes padding and masks last word of sentence. (EOS token)\n",
    "        :param batches: A tuple (input, question, label, mask)\n",
    "        :return A tuple (input, question, label, mask)\n",
    "        \"\"\"\n",
    "        \n",
    "        start_index = 0\n",
    "        end_index = 200\n",
    "        \n",
    "        inp = inputs[start_index:end_index]\n",
    "        num_word = num_words[start_index:end_index]\n",
    "        label = labels[start_index:end_index]\n",
    "        \n",
    "        B, V = batch_size, word_vector_size\n",
    "\n",
    "        L = 0\n",
    "        for n in range(B):\n",
    "            sent_len = len(inp[n])\n",
    "            L = max(L, sent_len)\n",
    "\n",
    "\n",
    "        new_input = np.zeros([B, L, V])  # zero padding\n",
    "        new_labels = []\n",
    "        new_num_words = []\n",
    "        for n in range(B):\n",
    "            new_input[n, :num_word[n]] = inp[n]\n",
    "            new_labels.append(label[n])\n",
    "            new_num_words.append(num_word[n])\n",
    "\n",
    "\n",
    "        return new_input, new_labels, new_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "ivocab = {}\n",
    "\n",
    "batch_size = 200\n",
    "hidden_size = 50\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "sentences, labels = read_story()\n",
    "inputs, num_words = make_input_vector(sentences, word2vec, vocab, ivocab, word_vector_size)\n",
    "vocab_size = len(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B, L = batch_size, max(num_words)\n",
    "V, h, A = word_vector_size, hidden_size, vocab_size\n",
    "\n",
    "inp, len_sentences, label = process_batch()\n",
    "inp = np.float32(inp)\n",
    "#feed_dict={inp:new_input, len_sentences:new_num_words, label:new_labels}\n",
    "\"\"\"\n",
    "inp = tf.placeholder('float32', shape = [B, L, V], name = 'x')\n",
    "len_sentences = tf.placeholder('int32', shape = [B], name = 'x_len')\n",
    "label = tf.placeholder('int32', shape=[B], name = 'y')\n",
    "\"\"\"\n",
    "gru = rnn_cell.GRUCell(h)\n",
    "outputs, final_states = tf.nn.dynamic_rnn(cell = gru, \n",
    "                                          inputs = inp, \n",
    "                                          sequence_length = len_sentences, \n",
    "                                          dtype = tf.float32)    \n",
    "\n",
    "with tf.name_scope('answer'):\n",
    "    w_a = weight('answer_weight', [h, num_classes] )\n",
    "    logits = tf.matmul(final_states, w_a)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('acc'):\n",
    "    predicts = tf.cast(tf.argmax(logits, 1), 'int32')\n",
    "    corrects = tf.equal(predicts, labels[:batch_size])\n",
    "    num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "opt_op = optimizer.minimize(loss) #, global_step = self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "\n",
    "    for i in range(10):\n",
    "        sess.run(opt_op)\n",
    "        a = sess.run(final_states)\n",
    "        acc_print = sess.run(accuracy)\n",
    "        loss_print = sess.run(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       ..., \n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan, ...,  nan,  nan,  nan]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
